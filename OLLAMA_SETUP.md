# Ollama Integration Setup Guide

## Overview
Your Nemo application now includes complete Ollama integration with bundled binary support and cloud fallback.

## ğŸš€ Features Implemented

### âœ… Bundled Ollama Support
- Ollama binary can be bundled with the desktop app
- No separate installation required for end users
- Automatic fallback to system Ollama if available

### âœ… Model Management
- **TinyLlama 1.1B** - Fast, lightweight (1GB)
- **Phi-3 Mini** - Balanced performance (2GB) 
- **BioMistral 7B** - Medical-focused (4GB)
- Hardware-based recommendations
- Download progress tracking

### âœ… Cloud Fallback
- **Google Gemini** as cloud backup
- Seamless switching between local and cloud models
- No interruption if local models fail

### âœ… Python Code Execution
- Backend API for secure code execution
- Full pandas, matplotlib, seaborn support
- 30-second execution timeout
- Proper error handling

## ğŸ”§ Setup Instructions

### 1. Environment Configuration
Add your Gemini API key to `.env` file:
```bash
NEXT_PUBLIC_GEMINI_API_KEY=your_api_key_here
```

Get your API key from: https://makersuite.google.com/app/apikey

### 2. Bundling Ollama (For Distribution)

To bundle Ollama with your desktop app:

#### Download Ollama Binaries
```bash
# Create resources directory
mkdir -p /app/src-tauri/resources/ollama

# Download for your platform:

# Windows
curl -L https://ollama.ai/download/ollama-windows-amd64.exe -o /app/src-tauri/resources/ollama/ollama.exe

# macOS
curl -L https://ollama.ai/download/ollama-darwin -o /app/src-tauri/resources/ollama/ollama

# Linux
curl -L https://ollama.ai/download/ollama-linux-amd64 -o /app/src-tauri/resources/ollama/ollama

# Make executable (Unix systems)
chmod +x /app/src-tauri/resources/ollama/ollama
```

#### Build Desktop App
```bash
# Install Tauri CLI
npm install -g @tauri-apps/cli

# Build the app (includes Ollama)
yarn tauri build
```

### 3. Development Mode

For development without bundled Ollama:

#### Option A: System Ollama
```bash
# Install Ollama system-wide
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service
ollama serve

# Download models
ollama pull tinyllama
ollama pull phi3:mini
```

#### Option B: Cloud-Only
- Set Gemini API key in `.env`
- Models will default to cloud fallback

## ğŸ® Usage

### Model Selection
1. Click "Setup Local AI" button
2. View system requirements and recommendations
3. Download recommended models based on your RAM
4. Models appear in dropdown when ready
5. Cloud models always available as fallback

### Chat Interface
- Select any model from dropdown (local or cloud)
- Ask questions about your uploaded data
- Get Python code generated by AI
- Execute code directly in the interface

### Model Recommendations
- **4GB RAM**: TinyLlama (fast, basic analysis)
- **6GB RAM**: Phi-3 Mini (balanced performance)
- **8GB+ RAM**: BioMistral (medical-focused analysis)

## ğŸ”§ Technical Details

### File Structure
```
/app/
â”œâ”€â”€ src-tauri/
â”‚   â”œâ”€â”€ resources/ollama/    # Bundled Ollama binaries
â”‚   â”œâ”€â”€ src/ollama.rs        # Rust Ollama management
â”‚   â””â”€â”€ tauri.conf.json      # Bundle configuration
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ollama-client.ts     # Frontend Ollama client
â”‚   â””â”€â”€ ai-service.ts        # Unified AI service
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ model-selector.tsx   # Model management UI
â”‚   â””â”€â”€ chat-panel.tsx       # Chat interface
â””â”€â”€ backend/
    â””â”€â”€ app.py               # Python execution API
```

### API Endpoints

Backend API (`/api/`):
- `POST /execute-python` - Execute Python code
- `GET /health` - Backend health check
- (All existing statistical analysis endpoints)

Tauri Commands:
- `get_hardware_info` - System specs
- `setup_bundled_ollama` - Initialize bundled binary
- `start_ollama` - Start Ollama service
- `download_model` - Download AI models
- `query_ollama` - Query local models

## ğŸš¨ Troubleshooting

### Issue: "Failed to start Ollama"
**Solution**: Ensure either:
1. Ollama binary is bundled in `resources/ollama/`
2. System Ollama is installed
3. Use cloud fallback (Gemini)

### Issue: "Model download failed"
**Solution**: 
1. Check internet connection
2. Ensure sufficient disk space (1-4GB per model)
3. Try restarting Ollama service

### Issue: "Cloud model not working"
**Solution**:
1. Verify `NEXT_PUBLIC_GEMINI_API_KEY` in `.env`
2. Check API key validity at Google AI Studio
3. Restart the application

### Issue: "Python code execution failed"
**Solution**:
1. Ensure backend is running (`sudo supervisorctl status`)
2. Check if pandas/matplotlib are installed
3. Code timeout is 30 seconds maximum

## ğŸ” Privacy & Security

### Local Models
- **Fully Offline**: Data never leaves your device
- **No Telemetry**: No usage tracking
- **Private Processing**: All analysis done locally

### Cloud Fallback
- **Google Gemini**: Data sent to Google AI
- **Configurable**: Can be disabled by not setting API key
- **Transparent**: Clear indication when using cloud models

## ğŸ“‹ Deployment Checklist

For production deployment:

- [ ] Bundle Ollama binaries for target platforms
- [ ] Test hardware recommendations on different systems
- [ ] Verify model downloads work offline
- [ ] Test cloud fallback functionality
- [ ] Validate Python code execution security
- [ ] Test with various dataset sizes
- [ ] Verify desktop app installer size (<50MB recommended)

## ğŸ¯ Next Steps

1. **Test the integration** with sample medical datasets
2. **Download models** based on your system specs
3. **Configure Gemini API** for cloud fallback
4. **Build desktop app** with bundled Ollama
5. **Deploy** to target users

The integration is now complete and ready for testing!